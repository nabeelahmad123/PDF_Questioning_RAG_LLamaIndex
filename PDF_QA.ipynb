{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='2376d156-adf6-4c4e-ac6d-ae4c1d92a49e', embedding=None, metadata={'page_label': '1', 'file_name': 'Langchain_paper_1.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_1.pdf', 'file_type': 'application/pdf', 'file_size': 326993, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Submitte d to the 3rd International Conference on “Wom en in Science & Technology: Creating Sustainable Career”  \\n28 -30 December, 2023  Automating Customer Ser vice using LangChain  \\nBuilding custom open -source GPT Chatbot for organizat ions \\nKeivalya Pandya  \\n19me439 @bvmengineering.ac.in  \\nBirla Vishvakarma Mahavidyalaya, Gujarat, I ndia Prof . Dr. Mehfuza Holia  \\nmsholia@bvmengineering.ac.in  \\nBirla Vishvakarma Mahavidyalaya, Gujarat, I ndia \\n \\nAbstract— In the digital age, the dynamics of customer \\nservice are evolving, driven by technological advancements and \\nthe integration of Large La ngua ge Models (LLMs). This resea rch \\npaper introduces a groundbreaking approach to automating \\ncustomer service using LangChain, a custom LLM tailored for \\norganizations. The paper explores the obsolescence of traditional \\ncustomer support techniques, particul arly Frequently Asked \\nQuestions (FAQs), and proposes a paradigm shift towards \\nresponsive, context -aware, and personalized customer \\ninteractions. The heart of this innovation lies in the fusion of \\nopen -source methodologies, web scraping, fine -tuning, and th e \\nseamless integration of LangCh ain into customer service \\nplatforms. This open -source  state -of-the-art framework, \\npresented as \"Sahaay,\" demonstrates the ability to scale across \\nindustries and organizations, offering real -time support and \\nquery resolution. Key  elements of this research encompass data \\ncollection via web scraping, the role of embeddings, the \\nutilization of Google\\'s Flan T5 XXL , Base and S mall language \\nmodel s for knowledge retrieval, and the integration of the \\nchatbot into customer service platfor ms. The results s ection \\nprovid es insights into the ir performance  and use cases, here \\nparticularly within an educational institution.  This research \\nheralds a new era in customer service, where technology is \\nharnessed to create effici ent, personalized, and r esponsive \\ninterac tions. Sahaay, power ed b y LangChain, redefines the \\ncustomer -company relationship, elevating customer retention, \\nvalue extraction, and brand image. As organizations embrace \\nLLMs , customer service bec omes a dynamic and customer -\\ncentric ecosy stem.  \\nKeywords — Customer  Service Automation, Large L anguage \\nModels, LangChain, Web Scraping, Context -Aware Interaction s \\nI.  INTRODUCTION  \\n“Customer is king” is the ancient  mantra reflecting the \\nsignificance of customers  in every business. In the digital age, \\nwhere the rhythms of mode rn life are guided by the pu lse of \\ntechnology, the realm of customer service stands as the \\nfrontline of engagement between businesses and their  clientele. \\nIt is the place where queries are an swered, problems are \\nresolved, and trust is forged.  \\nThis research  paper brings the future of customer service, \\nwhere automation, personalization, and responsiveness \\nconverge to redefine the customer -company r elationship. At \\nthe heart of this transformation  lies the integration of LLMs , \\nexemplifie d by LangChain  [1]. \\nIn the annal s of customer service history, FAQs  and \\ntraditional support mechanisms have long held sway. These \\nvenerable tools have dutifully served as repositories of \\ninformation, attempting to a ddress the queries and concerns of \\ncustom ers. However, as we stan d at the cusp of a new era in customer service automation, it becomes abundantly clear that \\nthe traditional methods once hailed as revolutionary, are \\ngradually becoming obsolete.  \\nThis paper i s an invitation to envision a future wher e \\ncustomer service is no t a cost center but a wellspring of \\ncustomer satisfaction and loyalty. We propose an open -source \\nframework that can be scaled to any industry or organization to \\nfulfill the consumer needs for  support and query resolution \\nwithin seco nds. \\nFor demonstration p urposes, we use the information \\npresented by Birla Vishvakarma Mahavidyalaya (BVM) \\nEngineering  College on their website \\nhttps://bvmengineering.ac.i n/ as the context for our chatbot, \\nfrom where it can retrieve al l the information in real -time and \\nanswer to any queries that are raised by the users. Here, users \\ncan be anyone ranging from prospective  students, current \\nstudents who intend to get informat ion from the Notice Board, \\nresearchers wh o wish to search for the ir potential research \\nguide, and so on. The applications are endless.  \\nII. LITERATURE SURVEY  \\nS. Kim (2023) et al addresses the challenge of deploying \\nresource -intensive large neural models, such a s Transformers, \\nfor information retrieval  (IR) while maintaining efficiency.  \\nExperimental results on MSMARCO benchmarks demonstrate \\nthe effectiveness o f this approach, achieving successful \\ndistillation of both dual -encoder and cross -encoder teacher \\nmodels into smaller, 1/10th size asymmetric stud ents while \\nretaining 95 -97% of the teacher\\'s performance  [2]. L. Bonifacio  \\net al (2022)  highlights the recent transformation in the \\nInformation Retrieval (IR) field, propelled by the emergence of \\nlarge pretrained tr ansformer models. The MS MARCO dataset \\nplayed a pivotal role in t his revolution, enabling zero -shot \\ntransfer learning across various tasks  [3]. \\nThis paper proposed a novel open -source ap proach to \\nbuilding LLM Chatbots using custom knowledge from the \\nconten t in the website.  It is unique i n several  ways: \\n1. We propose a n ope n-source framework which is robust \\nwith the type of dataset available on the webpage or \\nthe web of links.  \\n2. This implementation aims to compliment the use of \\nFAQs with a more interactive and us er-friendly  \\ninterface.  \\n3. We then do a c ompa rative study of various models, \\ntheir performance on  the provided data relative to the \\nexpected response from the LLM.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e1ddfd03-6911-46c0-a4d4-28978f66612c', embedding=None, metadata={'page_label': '2', 'file_name': 'Langchain_paper_1.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_1.pdf', 'file_type': 'application/pdf', 'file_size': 326993, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Submitte d to the 3rd International Conference on “Wom en in Science & Technology: Creating Sustainable Career”  \\n28 -30 December, 2023  III. METHODOLOGY  \\nThis sect ion covers the data c ollection, details about the \\nselected model, fine -tuning, and integration with the Gradio \\nAPIs for web deployment.  \\nA. Data Collection  \\nTo gather the necessary data for our project, we employed \\nBeautifulSoup  web scraping techniques to retri eve publicly \\naccessib le information from an organization’s homepage. We \\nobserved this page is often linke d with all  the relevant \\ninformation required for the user/visitor. This approach \\nallowed us to collect a wide array of data, including customer \\nservice  FAQs, product manual s, support forums, chat logs, \\nassociated institutions,  and so on. This data further serves as \\nthe context for our  LLM . \\nB. Embeddings  \\nEmbeddings play a pivotal role in the development of any \\nLLM  powered . They are vector representation s of words or \\nphrases in a continuous mathematical space that capture \\nsemantic a nd contextual information, allowing the model to \\nunderstand the meaning and re lationships between words, \\nwhich is essential for pro viding meaningful responses to user \\nqueries.  \\nWe have used HuggingFace Instuct Embeddings – \\n“hkunlp/instructor -large” a text e mbedding model fine -tuned \\nfor specific t asks and domains, such as classificati on, retrieval, \\nclustering, and text evaluation  [4]. What sets Instructor apart is \\nits ability to gener ate tailored text embeddings without \\nrequiring additional fine -tuning. These  embeddings are then \\nstored using FAISS  (Facebook AI  Similarity Search)  library \\nthat allows developers to quickly search for embedd ings of \\nmultimedia documents that are similar to each other  [5]. \\nC. Language M odel \\nWe have chosen Google’s Flan T5 XXL as the mo st \\napprop riate language model after comp aring with other F lan T5 \\ndistributions to retrieve knowledge from the vectorspace  and \\nchat_history  (or memory)  [6]. The model  retains the context of \\nprevious messages  and uses that as a reference to predict \\nanswers f or the upcoming questions. This helps us ers to have \\nan interactive conversation with the chatbot, instead of a \\nmonotonous and robotic one.  \\nD. Integration with Customer Service Platforms  \\nA simple chat window ca n be activated at the corner of any \\nwebsite which would enable users to interact with the chatbot \\nand ask any relevant questions or doubts regarding the \\norganization.  However, fo r the d emons tration purpose of this \\npaper, we are using Gradio API framework  [7]. \\nIV. RESULTS  \\nIn this section, we mention the metric s of comparison, \\nprovide comparative ana lysis, and use cases in associ ation with \\nan educational institution.  A. Evaluating the Performance of LLMs  \\nIt is relatively difficul t to evaluate LangChain agents, \\nespec ially when trained on large chunks of context data sets for \\ninformation retrieval. Hence, t he current solution for the lack of \\nmetrics is to rely on human knowledge to get a sense of how \\nthe chain/agent is performing.  \\nIt is e vident from TABLE -I, II and II I that the XXL model \\noutperforms other competitive LLMs  such as BASE and \\nSMALL.  \\n \\nFig. 1.  Mod el Architecture – Sahaay  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f267ef66-0b5d-4a1c-883c-85940d683d95', embedding=None, metadata={'page_label': '3', 'file_name': 'Langchain_paper_1.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_1.pdf', 'file_type': 'application/pdf', 'file_size': 326993, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Submitte d to the 3rd International Conference on “Wom en in Science & Technology: Creating Sustainable Career”  \\n28 -30 December, 2023  TABLE I.  Google ’s Flan-T5-XXL Performance  \\nMetrics  \\nPerformance  Sr. \\nNo. Query/Prompt  Answer  \\n1. What is BVM?  Birla Vishvakarma \\nMahavidyalaya  ✯✯✯✯ \\n2. Where is it?a Vallabh Vidyanagar, Gujarat \\nIndia  ✯✯✯ \\n3. What is IEEE \\nBVM?  Institute of Electrical and \\nElectronics Engineers Stu dent \\nBranch of BVM  ✯✯✯✯ \\n4. What is TRS \\nBVM?  BVM Student Chapter \\nassociated with Rob otics \\nSociety India  ✯✯✯✯ \\n5. Name the annual  \\nnewsletter ? Vishvakarma Magazine and \\nNewsletter  ✯✯✯✯ \\n6. Tell me \\nsomething about \\nICWSTCSC  The International Conference \\nfor Wome n in Science and \\nTechnology Creating  \\nSustainable Car eer is the 3rd \\nInternational Con ference \\nhappening in hybrid mode.  ✯✯✯✯✯ \\na. Follow-up quest ion \\nTABLE II.  GOOGLE ’S FLAN-T5-BASE PERFORMANCE  \\nMetrics  \\nPerformance  Sr. \\nNo. Query/Prompt  Answer  \\n1. What is BVM?  BVM is the first engineering \\ncollege of Gujarat State \\nestablished way back in 1948.  ✯✯✯✯ \\n2. Where is it?b VV Nagar, Gujarat, India  ✯✯✯ \\n3. What is IEEE \\nBVM?  first engineering college of \\nGujarat State established way \\nback in 19 48 ✯ \\n4. What is TRS \\nBVM?  CVM BVM Roboti cs Society \\nOf India  ✯✯ \\n5. Name t he annual  \\nnewsletter ? Vishvakarma Magazine  ✯✯✯✯ \\n6. Tell me \\nsomething about \\nICWSTCSC  BVM 3rd Internartional \\nConference  ✯✯✯✯ \\nb. Follow-up quest ion \\nTABLE III.  GOOGLE ’S FLAN-T5-SMALL  PERFORMANCE  \\nMetrics  \\nPerformance  Sr. \\nNo. Query/Prompt  Answer  \\n1. What is BVM?  BVM is the first Autonomous \\nEngineering institute of Gujarat \\nto obtain academic autonomy \\nfor all its UG & PG programs, \\nfrom University Grant \\nCommission (UGC).  ✯✯✯ \\n2. Where is it?c BVM Engineering College  ✯ \\n3. What is IEEE \\nBVM?  engineering college of Gujarat \\nState  ✯ \\n4. What is TRS \\nBVM?  BVM  ✯ \\n5. Name the annual  \\nnewsletter ? Campus Publications  ✯ \\n6. Tell me \\nsomething about \\nICWSTCSC  ICWSTCSC 2023 PMSSS \\nstudents' reporting after \\nadmission (AY: 2023 -24) at \\nBVM 3rd Internartional \\nConfere nce. ✯✯✯ c. Follow-up quest ion \\nB. Applications  \\nCustomer service establishes a direct connection between \\nthe customer and t he company. It  retains customers and \\nextracts higher value from t hem. By harnessing the power of \\nLarge Language Models  as shown in  Fig. 2 , customer service \\ncan be elevate d to new heights, facilitating efficient, \\npersonalized, and responsive interactions. The LangChain f ine-\\ntuned over custom knowledge of the product, service, or \\norganizati on can effectively address a wide array of customer \\ninquiries and issues. Its ability to understand context and \\nhistory empowers it to provide personalized support to \\ncustomers. Automate d cus tomer service powered by LLMs is \\navailable around the clock and i s also proficient in multiple \\nlanguages.  \\nV. CONCLUSION  \\nIn the ever -evolving landscape of  customer service, the \\nintroduction of Sahaay’s innovative approach presented in this \\npaper, using La ngChain as a prime example, ushered in a new \\nera of automation.  Auto mating customer se rvice using \\nSahaay’s open -source Large  Language architecture leveragi ng \\nLangChain revolutionizes the customer -company relationship \\nand CX. It enables companies to provide  efficient, \\n \\nFig. 2.  User interface  – Gradio framewor k \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0aaf9222-cf95-453c-be06-7135ca687823', embedding=None, metadata={'page_label': '4', 'file_name': 'Langchain_paper_1.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_1.pdf', 'file_type': 'application/pdf', 'file_size': 326993, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Submitte d to the 3rd International Conference on “Wom en in Science & Technology: Creating Sustainable Career”  \\n28 -30 December, 2023  personalized, and responsive support, ultimately leading  to \\ncustomer reten tion, increased customer value, and a more \\npositive brand image. As o rganizations continue to leverage the \\ncapabilities of LLMs, the landscape of customer service is \\nevolving into a more dynamic and customer -centric ecosystem.  \\nThis paper dem onstrates a comparis on between various \\nmodel  performa nces and evaluate s them on the basis of the \\nquality of response generated.  We ha ve compared \\nGOOGLE/FLAN -T5-XXL with GOOGLE/FLAN -T5-BASE , \\nand GOOGLE/FLAN -T5-SMALL  and observed that the XXL \\nmodel outperforms the other  LLMs in the pro vided task. Each \\nmodel is  posed with t he same questions . \\nIn the futu re, Sahaay can acc ess PDFs, Videos, Audio, and \\nother files to extract relevant informat ion about , for example, \\nstudent activi ties, research work and innovation carried out by \\nBVM . This multimodal capability has th e potential to change \\nforever the way we interact with websites  and retrieve \\ninformation in much less time.  \\nACKNOWLEDGMENT  \\nThe authors would like to express their deepest appreciation \\nto the research facili ty provided at TRS BVM Laboratory for \\nencouraging multi -disciplinary collaborative research within \\nthe campus. We ’d also like to thank Birla Vishvakarma \\nMahavidyalaya (E ngineering College) for allowing us to \\nexperiment with the innovation on their website.  REFERENCES  \\n[1] Asbjørn Følstad and Marita Skjuve. 201 9. Chatbots for customer \\nservice: user experience and motivatio n. In Proceedings of the 1st \\nInternational Conference on Conversational User Interfaces (CUI \\'19). \\nAssociation for Computing Machinery, New Yor k, NY, USA, Article 1, \\n1–9. https://doi.org/10.1145/3342775.3342784  \\n[2] Kim, S., Rawat, A. S., Zaheer, M., Jayasumana , S., Sadhanala, V., \\nJitkrittum, W., Menon, A. K., Fergus, R., & Kumar, S. (2023). \\nEmbedDistill: A Geometric Knowledge Distillation for Informa tion \\nRetrieval. ArXiv. /abs/2301.12005  \\n[3] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo \\nNogueira. 20 22. InPars: Unsupervised Dataset Generation for \\nInformation Retrieval. In Proceedings of the 45th International ACM \\nSIGIR Conference on Research a nd Development in Information \\nRetrieval (SIGIR \\'22). Association for Computing Machinery, New \\nYork, NY, USA, 2 387–2392. https://doi.org/10.1145/3477495.3531863  \\n[4] Su, Hongjin, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari \\nOstendorf, Wen Yih, Noah A. S mith, Luke Zettlemoyer, and Tao Yu. \\n\"One Embedder, Any Task: Instruction -Finetuned Text Embeddings.\" \\nArXiv, (2 022). /abs/2212.097 41. \\n[5] Johnson, Jeff, Matthijs Douze, and Hervé Jégou. \"Billion -scale \\nSimilarity Search with GPUs.\" ArXiv , (2017). Accessed Septem ber 28, \\n2023. /abs/1702.08734.  \\n[6] Chung, Hyung W., Le Hou, Shayne Longpre , Barret Zoph, Yi Tay, \\nWilliam Fedus, Yu nxuan Li et al. \"Scaling Instruction -Finetuned \\nLanguage Models.\" ArXiv, (2022). Accessed September 28, 2023. \\n/abs/2210.11416  \\n[7] Abid, A., Abdalla, A. , Abid, A., Khan, D., Alfozan, A., & Zou, J. \\n(2019). Gradio: Hassle -Free Sharing and Testing of ML Models in the \\nWild. ArXiv. /abs/1906.02569  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61ca5378-bc74-4c8b-a86d-790115bea6e1', embedding=None, metadata={'page_label': '1', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/372669736\\nCreating Large Language Model Applications Utilizing LangChain: A Primer on\\nDeveloping LLM Apps Fast\\nArticle \\xa0\\xa0 in\\xa0\\xa0International Conf erence on Applied Engineering and Nat ural Scienc es · July 2023\\nDOI: 10.59287/ icaens.1127\\nCITATIONS\\n29READS\\n15,217\\n2 author s:\\nOguzhan T opsak al\\nFlorida P olyt echnic Univ ersity\\n40 PUBLICA TIONS \\xa0\\xa0\\xa0292 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nT. Cetin Akinci\\nUniv ersity of Calif ornia, Riv erside\\n135 PUBLICA TIONS \\xa0\\xa0\\xa0934 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Oguzhan T opsak al on 07 A ugust 2023.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d75e84ef-094f-470a-ac75-2bfaac6b56d2', embedding=None, metadata={'page_label': '2', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nAll Sciences  Proceedings  \\nhttp://as -proceeding.com/  5th International Conference on Applied \\nEngineering and Natural Sciences  \\n \\nJuly 10 -12, 2023 : Konya, Turkey  \\n \\nhttps://www.icaens.com/  © 20 23 Published by All Sciences Proceedings  \\n \\n1050  \\n \\n \\nCreating Large Language Model Applications Utilizing LangChain: A \\nPrimer on Developing LLM Apps Fast  \\nOguzhan Topsakal1*, and Tahi r Cetin Akinci  2  \\n1Computer Science Department , Florida Polytechnic University, FL, USA  \\n2WCGEC , University  of California at Riverside , CA, USA  \\n*(otopsakal @floridapoly .edu) Email of the corresponding author  \\n \\nAbstract – This study focuses on the utilization of Large Language Models (LLMs) for the rapid \\ndevelopment of applications, with a spotlight on L angChain, an open -source software library. LLMs have \\nbeen rapidly adopted due to their capabilities in a range of tasks, including essay composition, code \\nwriting, explanation, and debugging, with OpenAI’s ChatGPT popularizing their usage among millions of  \\nusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke \\nAI applications using LLMs. LangChain has been widely recognized in the AI community for its ability \\nto seamlessly interact with various data sources an d applications. The paper provides a n examination of \\nLangChain's core features, including its components and chains, acting as modular abstractions and \\ncustomizable, use -case-specific pipelines, respectively. Through a series of practical examples, the stu dy \\nelucidates the potential of this framework in fostering the swift development of LLM -based applications .   \\n \\nKeywords – Large Language Models, LangChain, Concepts, Application,  ChatGPT, NLP, GPT  \\n \\nI. INTRODUCTION  \\nThe past decade has witnessed an unparalleled \\nevolution in the realm of artificial intelligence (AI). \\nThis period, characterized by the ascendancy of \\ndeep learning through the utilization of neural \\nnetworks, has resulted in significant enhancements \\nin capabilities pertaining to image and speech \\nrecognition. One salien t milestone highlighting this \\nprogress is the ImageNet Large Scale Visual \\nRecognition Challenge, which effectively \\ndemonstrated the prowess of AI capabilities in \\nimage recognition . [1][2]  \\nAnother major milestone in the AI landscape is \\nthe successful implem entation of reinforcement \\nlearning, as exemplified by DeepMind's AlphaGo \\nand AlphaZero.  [3] These innovations have \\ndemonstrated extraordinary performance in \\ncomplex games, such as Go and Chess, using self -\\nplay algorithms, thereby signifying a leap forward \\nin reinforcement learning techniques. In parallel, \\nthe evolution of generative models has facilitated the creation of convincingly realistic synthetic \\nmultimedia content . \\nDuring the same period, the field of natural \\nlanguage processing (NLP) experienced \\nremarkable transformations. The advent of \\nadvanced models, exemplified by the likes of \\nBERT (Bidirectional Encoder Representations \\nfrom Transformers ) by Google [4] and GPT \\n(Generative Pretrained Transformer ) by OpenAI  \\n[5], and T5 (Text -to-Text Transfer Transformer) by \\nGoogle  [6] has fostered significant improvements \\nin machine translation, sentiment analysis, and text \\ngeneration, thus ushering in a new era for NLP.  \\nBERT, GPT,  T5 and similar technologies all \\nutilized transformers architecture and were trained \\non huge amount of data and hence named  as Large \\nLanguage Models (LLMs).  [7] As LLMs were \\ntrained using more and more data, and \\nencompassed more parameters, their capabilit ies \\nincreased. For example, GPT -1 (June 2018 ), GPT -\\n2 (February 2019 ), and GPT -3 (June 2020 ) had 117 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c9dd6fe-f7ac-4781-8372-2e81401d39d5', embedding=None, metadata={'page_label': '3', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n1051  \\n million , 1.5 billion , and 175 billion  parameters , \\nrespectively.  \\nA large language model (LLM) is a subtype of \\nartificial intelligence model that generates t ext with \\nhuman -like proficiency.  [7] Characterized by a \\nsizable number of parameters and trained on \\nexpansive text corpora, these models are equipped \\nto produce contextually pertinent and \\ngrammatically coherent outputs.  \\nUtilizing machine learning techniques such as \\ndeep learning, these models are trained to predict \\nsubsequent words in a sentence based on prior \\ncontext, thereby enabling the generation of \\ncomprehensive sentences and paragraphs that bear \\na resemblance to hum an-authored text.  \\nEven though  the LLMs  present limitations , such \\nas occasionally producing erroneous or illogical \\noutputs  (also called hallucination ), they achieved \\nrapid success due to their performance in doing \\nvarious tasks such as compos ing essays, writing, \\nexplaining, and debugging code. The recent \\nOpenAI’s LLM, ChatGPT, made the technology \\nknown to most, acquiring millions of users in a \\nshort am ount of time. The capabilities of GPTs \\nhave become even more impressive with the \\nrelease of GPT4. [ 8]. \\nMany started to think  about how  to leverage this \\ntechnology to provide solutions for fields like \\neducation , research, c ustomer service , content \\ncreation , healthcare , entertainment, etc.  It became \\npossible to develop AI applications much faster \\nthan ever before b y interacting with  an LLM.  \\nHowever, custom AI apps require more than just \\ninteracting via a web interface. A recent open -\\nsource software library called LangChain, started \\nproviding solutions for the steps of developing a \\ncustom AI app utilizing LLMs and gained much \\nattention from th e AI community.  [9][10] In this \\narticle, we describe the capabilities of LangChain \\nand provide a primer on developing  large language \\nmodel applications rapidly u tilizing LangChain . \\nII. MATERIALS AND METHOD  \\nLangChain is a framework for developing \\napplications u tilizing large language models , and \\nits goal is to enable developers to conveniently \\nutilize other data sources and interact with other \\napplications. To enable this, LangChain provides \\ncomponents (modular abstractions) and chains \\n(customizable use case-specific pipelines). We first provide an overview of components and then \\ndescribe several use cases .  \\nA. Components  \\nNext, the main components of LangChain, such \\nas Prompts, Memory, Chains, and Agents are \\nexplained.  \\nA.A.1  Prompts  \\nA \"prompt\" is the input to a LLM. They are \\ngenerally generated dynamically when used in an \\nLLM application and includes user’s input  \\n(question), a set of few shot examples to help the \\nlanguage model generate a better response , and \\ninstructions for the LLM regarding how  to process \\nthe input that comes from the user. LangChain \\nprovides several classes to construct prompts \\nutilizing several specialized Prompt  Template s. A \\nprompt template refers to a reproducible way to \\ngenerate a prompt. It contains a text string (\"the \\ntemplate\") that can take in a set of parameters from \\nthe end user and generates a prompt.  \\nTable 1. Example s of prompts. The examples are adapted \\nfrom the DeepLearning.AI course on LangChain.  \\nTask  Prompt  \\nExtracting \\ninformation   For the following text, extract the \\nfollowing information:  \\ngift: Was the item purchased as a gift for \\nsomeone else? Answer True if yes, False \\nif not or unknown.  \\ndelivery_days: How many days did it \\ntake for the product to arrive? If this \\ninformation is not found, output -1. \\nprice_ value: Extract any sentences about \\nthe value or price.  \\ntext: {text}  \\nWriting a \\nresponse  Write a follow -up response to the \\nfollowing summary in the specified \\nlanguage:  \\nSummary: {summary}  \\nLanguage: {language}  \\n \\nA.A.2  Models  \\nLarge Language Models  (LLMs ) are the main \\ntype of models utilized in LangChain. They accept \\na text string (prompt) and output a text string. \\nThere are other types of models that are used in \\nLangChain, namely, Chat Models  and Text \\nEmbedding  Models . Chat Models have more \\nstructured AP I processing chat messages , and text \\nembedding models take text and return its \\ncorresponding embedding as a list of floats.  The \\nembeddings are required when we want to work ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62a7a680-61b6-4c7e-9d24-28bc810fb988', embedding=None, metadata={'page_label': '4', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n1052  \\n with our own documents , as discussed in the \\nfollowing Question Answering from Docum ents \\nsection.  \\nA.A.3  Chains  \\nThe most important key building block of \\nLangChain  is the chain. The chain usually \\ncombines an LLM  together with a prompt, and \\nwith this building block , you can also put a bunch \\nof these building blocks together to carry out a \\nsequence  of operations on your text or on your \\nother data.  \\nA simple chain takes one input prompt and \\nproduces an output. Multiple  chains can be run one \\nafter another, where the output of the first chain  \\nbecomes the input of the next chain. Multiple \\nchains can be c oncatenated using the Simple \\nSequential Chain  class when there is one input and \\none output , as illustrated in Fig 1 .  \\n \\n \\nFig. 1 Example of a  Simple Sequential Chain  \\nLangChain provides another class named \\nSequentialChain, when there can be multiple inputs \\nbut one output, as illustrated in Fig 2.  \\n \\nFig. 2 Example of a  Sequential Chain that gets two inputs and \\noutputs on e result . \\nA pretty common scenario  is to use several \\nchains and to route an input to a chain depending \\non what the input is. For example, if you have \\nmultiple chains, each of which specialized for a \\nparticular type of input, you could have a router \\nchain which first decides which subchain to pass it \\nto and then passes it to that chain.  An example of \\nrouter chain is depicted in F ig 3. \\n  \\nFig. 3 Example of a  Router Chain that routes the input to a \\nchain that can answer the input the best.  \\nEach chain can utilize a different or the same \\nLLM and would be differentiated mostly based on \\ntheir prompt.  The prompts that go into an LLM as \\ninput include a description of the role of the chain. \\nThe input coming from the user is combined with \\nthe description of the expected output to form the \\nprompt so that the chain behaves as expected. \\nTable 1 gives examples of these prompt \\ndescriptions that a re used along with the user \\ninputs to let LLM produce a result.  \\nTable 2. Example s of chain prompts. The examples are \\nadapted  from the DeepLearning.AI course on LangChain . \\nName  Description  \\nMath  You are a very good mathematician. You are \\ngreat at answering math questions. You are \\nso good because you can break down hard \\nproblems into their component parts, answer \\nthe component parts, and then put them \\ntogether to answer the broader question.  \\nHistory  You are a very good historian. You have an \\nexcell ent knowledge of and understanding of \\npeople, events, and contexts from a range of \\nhistorical periods. You can think, reflect, \\ndebate, discuss , and evaluate the past. You \\nhave respect for historical evidence and the \\nability to make use of it to support you r \\nexplanations and judgments.  \\n \\nA.A.4  Memory  \\nThe language model itself is stateless and does \\nnot remember the conversation you've had so far. \\nEach transaction (call) to the LLM’s API endpoint \\nis independent. The illusion of memory in chatbot \\nsystems is facilitat ed by supplementary  code, \\nwhich incorporates the context of preceding \\ndialogues when interacting with the LLM . A \\nmemory  component  is needed  that can store the \\nprevious conversations and pass them to the LLM  \\nwith the next prompt .  \\nIn the LangChain framework, memory \\nimplementation can take multiple forms. The \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c57f1d37-2c5e-427f-bb33-fe176bf0cc6d', embedding=None, metadata={'page_label': '5', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n1053  \\n 'buffer' type delineates memory bounds based on a \\nset quantity of conversational exchanges, whereas \\nthe 'token' type imposes limitations contingent on \\nthe number of tokens. The 'summary memory' type \\napplies an abstracted summary of tokens when a \\ncertain threshold is exceeded. Beyond these \\nmemory types, developers have the discretion to \\narchive the entire conversation within a \\nconventional database or a key -value store. This \\ncan be beneficial for auditing purposes or to \\nenhance system performance through reflective \\nanalysis of past interactions.  \\nA.A.5  Question Answering from Documents  \\nOne of the most common  and complex \\napplications that are being built  using an LLM is a \\nsystem that can answer questions o n top of or \\nabout a document. Given a piece of text, extracted \\nfrom a PDF file, a webpage, a csv file or another \\ntype of document, apps can be developed to use an \\nLLM to answer questions about the content of \\nthose documents to help users gain a deeper \\nunde rstanding and get access to the information \\nthat they need. Once LLMs can be utilized to \\nprocess data that they were not originally trained \\nfor, there are many use cases that can be achieved.  \\nThere are various steps involved in answering  \\nquestions from own  documents , as depicted in Fig \\n4.  \\n \\n \\nFig. 4 Steps to answer questions from own documents.  \\n \\nLangChain provides  functionality to easily achieve \\nthese steps to load, transform  (split) , store , and \\nquery your data . LangChain has the following \\nclasses that help  to perform these functionalities.  \\n• Document Loaders:   Load s documents from \\nmany different sources , including CSV, PDF, \\nHTML, JSON, Excel, GitHub, Google Drive, \\nOne Drive, XML, Wikipedia, and many more.  \\n• Document transformers:  Split s documents  into \\nsmaller chunks so that they can be processed by \\nLLMs.  \\n• Text embedding models : Take unstructured text \\nand turn it into a list of floating -point  numbers  \\nthat represent corresponding embeddings.  • Vector stores : Helps to s tore and search over \\nembedded data . \\n• Retrievers : Helps to q uery your data  based on \\nembedding similarities.  \\n \\nEmbeddings generate quantitative representations \\nfor textual units, encapsulating their semantic \\nessence in a numerical format. When applied to \\nsimilar textual content, these embeddin gs yield \\nclosely aligned vectors. This enables an evaluation \\nof textual similarity within the vector space, \\nfacilitating an intuitive understanding of textual \\ncoherence. This becomes particularly instrumental \\nwhen selecting text pieces to feed into a langu age \\nmodel for query resolution.   \\nThese embeddings are stored in vector databases. \\nWhen a user query comes in, it is converted into \\nembeddings and then sent to the vector database to \\nfind the most semantically close text based on the \\nsimilarity scores compu ted by getting a dot product \\nof the query embedding and the embeddings stored \\nin the vector database.  The dot product can also be \\nexpressed as the product of the magnitudes of the \\nvectors and the cosine of the angle between them , \\nas shown in the following equation, where a and b \\nare the vectors being compared  [12]: \\na ⋅ b = ∣a∣ ∣b∣ cosα \\n \\nThere are several methods to pass the content of \\nthe documents and process them via LLM. The \\nmost used method is the stuff method which is \\nsimple and works well if the text chunks that were \\nreturned from the vector store similarly function fit \\ninto the con text size of the LLM. The Fig 5. depicts \\nthe stuff method  where retrieved similar documents \\nare passed to LLM in a single prompt.  \\n \\n \\nFig. 5 Stuff method passes similar documents in a single \\nprompt . \\nThe other methods that can be used in processing \\ndocuments when the retrieved documents  do not fit \\nin a single prompt are “map reduce ,” “refine ,” and \\n“map rerank .” \\nMap reduce takes all the chunks, passes them \\nalong with the question to a language model, gets \\nback a response, and then uses another language \\nmodel call to summarize all of the individual \\nresponses into a final answer. Map reduce  can \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03f8f0de-93d3-4a7c-8aa7-73b1d788450c', embedding=None, metadata={'page_label': '6', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n1054  \\n operate over any number of docum ents and can \\nprocess  individual questions in parallel. However, \\nit requires  more calls  and does treat all the \\ndocuments as independent, which may not always \\nbe the most desired thing.  Map reduce process is \\ndepicted in Figure 6.  \\n \\n \\nFig. 6 Map reduce passes similar documents to multiple \\nLLMs and then uses another LLM to finalize the output.  \\nRefine  is used to loop over many documents \\niteratively , building  upon the answer from the \\nprevious document , which generally leads to \\nlonger answers.  However, many calls a re required, \\nand the calls cannot be done in parallel , causing \\nslower execution . The refine methods is depicted in \\nFig 7.  \\n \\n \\nFig. 7 Refine method iteratively build s the answer . \\n \\nMap rerank do es a single call to the language \\nmodel for each document and ask s for a score. \\nThen it select s the document with the highest \\nscore.  The map rerank method is shown in Fig. 8.  \\n \\n \\nFig. 8 Map rerank method gets an output score and then \\nselects the output with th e highest score.  \\nA.A.6  Agents  \\nWhen an app needs a flexible chain of calls to \\nLLMs and other tools based on user input, agents \\ncan be utilized. An agent determines which tool \\nfrom a suite of tools needs to be used for the user \\ninput.  An agent can be either an action agent  that \\ndecides on the next action using the outputs of all \\nprevious actions or a plan-and-execute agent  that decide on the full sequence of actions upfront, then \\nexecute them all without updating the plan.  \\n• Action Agent operates at a high level by \\nreceiving user input, determining the \\nappropriate tool and its input, executing the \\ntool and recording its output (termed as an \\n'observation'), and making decisions on \\nsubsequent steps based on the history of tool \\nusage, inputs, and observations. This cycl e \\nrepeats until the agent can directly respond to \\nthe user. These agents are encapsulated in agent \\nexecutors that manage the sequence of actions \\nand interactions with the tools.  \\n• Plan and Execute Agent operates at a high level \\nby receiving user input, devis ing a \\ncomprehensive sequence of steps, and \\nimplementing these steps in a sequential \\nmanner, where outputs from previous steps are \\nutilized as inputs for subsequent ones. A \\ncommon implementation involves using a \\nlanguage model as the planner and an action \\nagent as the executor.  \\n \\nWithin the framework of agent -based systems, \\nthe concepts of tools and toolkits play a significant \\nrole. Tools, defined as interfaces that facilitate \\nagent -world interactions, constitute specific actions \\nthat an agent can perform, purposefully selected \\nbased on the agent's functional objective. \\nConversely, toolkits represent aggregated \\ncollections of synergistic tools, assembled to cater \\nto particular use cases. Toolkits are specifically \\ndesigned to consolidate tools that f unction \\neffectively in unison for distinct tasks, and they \\nencompass convenience methods for easy loading. \\nFor instance, an agent interfacing with a SQL \\ndatabase would necessitate a tool for executing \\nqueries and another for inspecting tables.  \\nB. Use Cases  \\nLangChain framework provides w alkthroughs of \\ncommon end -to-end use cases  on the topics such as \\nautonomous agents , chatbots , code understanding  \\nagents, e xtraction , question answering over \\ndocuments , s ummarization , and a nalyzing \\nstructured data . Each of these categories provides \\nseveral examples of how to utilize LangChain to \\nimplement the LLM app using Langchain. For \\nexample, the AutoGPT  sample given under the \\n‘autonomous agents ’ category  provides a notebook \\nimplementing AutoGPT using  LangChain  that \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfb1ffef-f5b7-4cf2-a944-e6b39c00966e', embedding=None, metadata={'page_label': '7', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n1055  \\n aims to autonomously achieve whatever goal is \\ngiven.  An increasing  number of examples of \\nLangChain use cases are documented at the \\nLangChain website. [12]  \\nIII. DISCUSSION  \\nThe rise of Large Language Models (LL Ms), \\nspecifically OpenAI's ChatGPT, signifies a \\ntransformative phase in AI development with \\nbroad -ranging applications. LangChain, an open -\\nsource library, stands out due to its proficiency in \\nintegrating with diverse data sources and \\napplications, making i t an influential tool in the AI \\ncommunity.  \\nLangChain's modular structure, with \\ncustomizable pipelines for specific use cases, \\nexpedites the development process for LLM \\napplications. This paper  contributes to the \\ndiscourse on LLM application development, \\naiming to spur further exploration of LangChain \\nand similar tools.  To help facilitate the \\ndevelopment of LLM applications utilizing \\nLangChain, we provide a sample Jupyter Notebook \\npage showcasing many of the concepts described \\nhere at a GitHub page. [13]  \\nIV. CON CLUSION  \\nIn conclusion, the advent and rapid adoption of \\nLarge Language Models, underscored by the rise of \\nOpenAI’s ChatGPT, signify a new frontier in the \\nAI landscape. These models have exhibited \\nproficiency across a multitude of tasks, setting a \\nprecedent  for future advancements.  \\nParticularly, LangChain, with its ability to \\nstreamline the development process of LLM \\napplications, has demonstrated significant potential \\nin the AI ecosystem. It has pioneered a new \\napproach that enables developers to interact with \\nvarious data sources and applications effortlessly. \\nThis flexibility and efficiency, characterized by \\nLangChain's modular abstractions and \\ncustomizable use -case-specific pipelines, make it \\nan invaluable tool for the future of LLM \\napplications.  \\nThis pa per provides insights into LangChain's \\nstructure and usage in specific use -cases, \\nelucidating its capacity to foster rapid \\ndevelopment. It is hoped that the potential and \\ncapabilities identified will spur more exploration \\nand innovation in the field of LLM s and serve as a \\nbasis for the development of more sophisticated applications, thus expanding the boundaries of \\nwhat is possible with artificial intelligence.  \\nREFERENCES  \\n \\n[1] OlgaRussakovsky, JiaDeng, HaoSu, JonathanKrause, \\nSanjeevSatheesh, SeanMa, Zhiheng, Hu ang, \\nAndrejKarpathy, AdityaKhosla, MichaelBernstein, et al. \\nImagenet large scale visual recognition challenge. IJCV, \\n2015.  \\n[2] Krizhevsky, Alex & Sutskever, Ilya & Hinton, \\nGeoffrey. (2012). ImageNet Classification with Deep \\nConvolutional Neural Networks. Neura l Information \\nProcessing Systems. 25. 10.1145/3065386 . \\n[3] Sean D. Holcomb, William K. Porter, Shaun V. Ault, \\nGuifen Mao, and Jin Wang. 2018. Overview on \\nDeepMind and Its AlphaGo Zero AI. In Proceedings of \\nthe 2018 International Conference on Big Data and \\nEducation (ICBDE '18). Association for Computing \\nMachinery, New York, NY, USA, 67 –71. \\nhttps://doi.org/10.1145/3206157.3206174  \\n[4] Jacob Devlin, Ming -Wei Chang, Kenton Lee, and \\nKristina Toutanova. 2019. BERT: Pre -training of Deep \\nBidirectional Transformers for Language \\nUnderstanding. In Proceedings of the 2019 Conference \\nof the North American Chapter of the Association for \\nComputational Linguistics: Human Language \\nTechnologies, Volume 1 (Long and Short Papers), \\npages 4171 –4186, Minneapolis, Minnesota. Associatio n \\nfor Computational Linguistics.  \\n[5] Radford, A., Narasimhan, K., Salimans, T. & Sutskever, \\nI. (2018). Improving language understanding by \\ngenerative pre -training.  \\n[6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine \\nLee, Sharan Narang, Michael Matena, Yanqi Zh ou, Wei \\nLi, & Peter J. Liu (2020). Exploring the Limits of \\nTransfer Learning with a Unified Text -to-Text \\nTransformer. Journal of Machine Learning Research, \\n21(140), 1 -67. \\n[7] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, \\nY., ... & Wen, J. R. (2023). A survey of large language \\nmodels. arXiv preprint arXiv:2303.18223  \\n[8] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., \\nHorvitz, E., Kamar, E., ... & Zhang, Y. (2023). Sparks \\nof artificial general intelligence: Early experiments with \\ngpt-4. arXiv preprin t arXiv:2303.12712.  \\n[9] Chase, H. LangChain LLM App Development \\nFramework. https://langchain.com/  Accessed Jul 10th, \\n2023  \\n[10] Chase, H. LangChain , Building applications with LLMs \\nthrough composability , GitHub Repo, \\nhttps://github.com/hwchase17/langchain  Accessed Jul \\n10th, 2023  \\n[11] Vector Similarity Explained, The Pinecone Vector DB, \\nhttps://www.pinecone.io/learn/vector -similarity/ , \\nAccessed July 10th, 2023.  \\n[12] LangChain Use Case Examples, \\nhttps://docs.langchain.com/docs/cat egory/use -cases  , \\nAccessed July 10th, 2023  \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b94c2b92-d808-4685-8ae6-fd6d87a058cc', embedding=None, metadata={'page_label': '8', 'file_name': 'Langchain_paper_2.pdf', 'file_path': 'c:\\\\Users\\\\Awais\\\\Desktop\\\\GenAI\\\\RAG_using_lamaindex\\\\data\\\\Langchain_paper_2.pdf', 'file_type': 'application/pdf', 'file_size': 591118, 'creation_date': '2024-05-18', 'last_modified_date': '2024-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n1056  \\n [13] GitHub page for sample Jupyter Notebook page \\nshowcasing usage of LangChain framework, \\nhttps://github.com/research -outcome/llm -langchain -\\nexamples  Accessed Jul 10th, 2023  \\n \\n \\n \\nView publication stats', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ec05039ba14507a7a2100d09e93bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2856adafe349cab890eca15d2712b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index= VectorStoreIndex.from_documents(documents,show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are a subtype of artificial intelligence models that are characterized by a significant number of parameters and are trained on extensive text corpora. These models are designed to generate text with human-like proficiency by predicting subsequent words in a sentence based on prior context. Despite occasional limitations such as producing erroneous outputs, LLMs have gained rapid success in various tasks like composing essays, writing, explaining, and debugging code. They have become popular tools for applications in fields such as education, research, customer service, content creation, healthcare, and entertainment.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are LLMS\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index= index, similarity_top_k=4)\n",
    "\n",
    "postprocessor= SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "query_engine=RetrieverQueryEngine(retriever= retriever,\n",
    "                                  node_postprocessors=[postprocessor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain enables the development of fast applications.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"does langchain creates fast applications?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: LangChain enables the development of fast\n",
      "applications.\n",
      "______________________________________________________________________\n",
      "Source Node 1/1\n",
      "Node ID: 017cdb70-7c59-4c7d-9fce-de14e058160a\n",
      "Similarity: 0.8068953725010856\n",
      "Text: See discussions, st ats, and author pr ofiles f or this public\n",
      "ation at : https://www .researchgate.ne t/public ation/372669736\n",
      "Creating Large Language Model Applications Utilizing LangChain: A\n",
      "Primer on Developing LLM Apps Fast Article    in  International Conf\n",
      "erence on Applied Engineering and Nat ural Scienc es · July 2023 DOI:\n",
      "10.59287/ icae...\n",
      "LangChain enables the development of fast applications.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a key building block of LangChain that combines a Large Language Model (LLM) with a prompt. It allows for a sequence of operations on text or other data by linking multiple chains together. Chains can be concatenated using classes like Simple Sequential Chain or SequentialChain, depending on the number of inputs and outputs required. Additionally, a router chain can be used to direct inputs to specialized subchains based on the type of input. Each chain in LangChain can utilize different or the same LLM, with prompts describing the role of the chain to guide the input for the model.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is blockchain\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are advanced models used in natural language processing (NLP) that utilize a specific architecture to process and generate text. These models, such as BERT, GPT, and T5, have been trained on large amounts of data and have significantly improved machine translation, sentiment analysis, and text generation tasks in the field of NLP.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are transformers?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
